# Crys_Rep_Enrich
## Enriching FTCP Crystal Representations via Multi-Modal Self-Supervised Learning

---

## üìò Overview

This repository provides the codebase, evaluation scripts, and experimental structure for the paper:

> **Enriching FTCP Representation of Crystals through Multi-Modal Self-Supervised Learning for Enhanced Materials Property Prediction**

The work introduces a hierarchical multi-modal self-supervised learning (SSL) framework that enriches the high-dimensional Fourier-Transformed Crystallographic Properties (FTCP) representation into compact, information-dense features without using any property labels.

The framework decomposes FTCP into six physically motivated modalities, trains specialized neural architectures via 13 self-supervised pretext tasks, and produces enriched representations that generalize across downstream materials property prediction tasks.

---

## üë• Authors

- **Danial Ebrahimzadeh** (University of Oklahoma) ‚Äî [danial.ebrahimzadeh@ou.edu](mailto:danial.ebrahimzadeh@ou.edu)
- **Sarah Sharif** (University of Oklahoma)
- **Yaser Mike Banad** (University of Oklahoma) ‚Äî Corresponding Author [bana@ou.edu](mailto:bana@ou.edu)

---

## üìÑ Abstract

Accelerating the discovery of functional inorganic materials requires machine learning models that can extract physically meaningful patterns from crystallographic data without relying on costly labeled property measurements. High-dimensional crystallographic representations such as the Fourier-Transformed Crystallographic Properties (FTCP) are statistically intractable for direct machine learning due to various challenges such as extreme dimensionality, heterogeneous sparsity, and redundant features. We introduce a hierarchical multi-modal self-supervised learning framework that decomposes FTCP into six physically motivated modalities and trains specialized neural architectures through 13 pretext tasks, producing enriched representations without requiring any property labels. Applied to 129,473 inorganic crystalline materials, the framework achieves a **12.3√ó compression** while demonstrating strong generalization as six of thirteen pretext tasks show test performance equal to or exceeding validation performance. Downstream evaluation across three complementary scenarios establishes the practical value of the enriched features. With simple linear regression, enriched features achieve stable formation energy RMSE of 0.826 eV/atom across all data splits while raw FTCP fails catastrophically, with a 37‚Äì40√ó computational speedup. Feature attribution via SHAP reveals that SSL features contribute 32.81% of predictive importance in a hybrid model despite comprising only 7.5% of feature dimensions, confirming non-redundant complementarity with raw FTCP. Under extreme data scarcity (1% of training data), a regularized residual model using enriched features achieves meaningful property prediction improvements, demonstrating effective transfer learning with 1% labeled samples. These results demonstrate that self-supervised data enrichment is a practical and principled pathway toward general-purpose crystallographic representations for high-throughput materials discovery.

---

## üìú License

This repository is released under the **MIT License**.

---

## üî¨ Key Contributions

- Multi-modal SSL framework tailored to crystallographic data
- **12.3√ó dimensionality reduction** (25,200 ‚Üí 2,048) without information loss
- Six specialized neural architectures aligned with physical structure
- 13 physics-informed pretext tasks (masking, reconstruction, consistency)
- Robust downstream evaluation across three complementary scenarios:
  - Linear regression stability
  - Hybrid FTCP + SSL interpretability (SHAP)
  - Extreme low-data transfer learning (1% labeled data)

---

## üìÇ Repository Structure

```
Crys_Rep_Enrich/
‚îÇ
‚îú‚îÄ‚îÄ Data/                       # (Empty placeholders ‚Äì see Dataset section)
‚îÇ   ‚îú‚îÄ‚îÄ FTCP/
‚îÇ   ‚îú‚îÄ‚îÄ Labels/
‚îÇ   ‚îî‚îÄ‚îÄ Splitted_Data/
‚îÇ       ‚îú‚îÄ‚îÄ Split_10Test_90Train/
‚îÇ       ‚îú‚îÄ‚îÄ Split_20Test_80Train/
‚îÇ       ‚îú‚îÄ‚îÄ Split_30Test_70Train/
‚îÇ       ‚îî‚îÄ‚îÄ Split_40Test_60Train/
‚îÇ
‚îú‚îÄ‚îÄ SSL_Training/               # Self-supervised pretraining (six FTCP blocks)
‚îÇ   ‚îú‚îÄ‚îÄ Block1_Element_Matrix/
‚îÇ   ‚îú‚îÄ‚îÄ Block2_Crystal_System/
‚îÇ   ‚îú‚îÄ‚îÄ Block3_Atomic_Sites/
‚îÇ   ‚îú‚îÄ‚îÄ Block4_Site_Occupancy/
‚îÇ   ‚îú‚îÄ‚îÄ Block5_Reciprocal_Space/
‚îÇ   ‚îî‚îÄ‚îÄ Block6_Structure_Factors/
‚îÇ
‚îú‚îÄ‚îÄ Evaluation_Scenarios/       # Downstream evaluation framework
‚îÇ   ‚îú‚îÄ‚îÄ Scenario1_Linear_Regression/
‚îÇ   ‚îú‚îÄ‚îÄ Scenario2_Hybrid_Enrichment/
‚îÇ   ‚îî‚îÄ‚îÄ Scenario3_Sample_Efficiency/
‚îÇ
‚îú‚îÄ‚îÄ Supplemental Information/   # Figures and additional results
‚îÇ
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üì¶ Dataset Access

> ‚ö†Ô∏è The `Data/` folders in this repository are intentionally empty.

Due to the large size of the dataset (FTCP tensors, splits, and labels), all data is hosted externally.

**üîó Download Dataset:**
üëâ [Hugging Face Dataset Repository](https://huggingface.co/datasets/danial199472/Crys_Rep_Enrich)

After downloading and extracting, place the contents into the corresponding `Data/` subfolders without modifying the directory names.

---

## üß† FTCP Modalities and SSL Blocks

| Block | FTCP Modality | Physical Meaning |
|-------|--------------|-----------------|
| 1 | Element Composition Matrix | Chemical identity & sparsity |
| 2 | Lattice Parameters | Symmetry & geometry |
| 3 | Atomic Sites | Local coordination & packing |
| 4 | Site Occupancy | Disorder & partial occupancy |
| 5 | Reciprocal Space | k-point topology |
| 6 | Structure Factors | Diffraction & Fourier physics |

Each block is trained independently using tailored architectures and physics-informed self-supervised objectives.

---

## ‚öôÔ∏è Installation

```bash
pip install -r requirements.txt
```

### GPU Support

For CUDA-enabled PyTorch, follow instructions at: https://pytorch.org/

Example:

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

---

## üíª System Recommendations

- **Python:** 3.8+
- **RAM:** ‚â• 32 GB (recommended)
- **GPU:** Optional, recommended for SSL training
- **Storage:** ‚â• 150 GB for full dataset and outputs

---

## üì¨ Contact

**Corresponding Author:**
Yaser Mike Banad ‚Äî [bana@ou.edu](mailto:bana@ou.edu)

**First Author:**
Danial Ebrahimzadeh ‚Äî [danial.ebrahimzadeh@ou.edu](mailto:danial.ebrahimzadeh@ou.edu)
